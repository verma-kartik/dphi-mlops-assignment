{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15432f4c-5573-4e7b-957e-363fe9b76851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admin\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "client = kfp.Client()\n",
    "NAMESPACE = client.get_user_namespace()\n",
    "EXPERIMENT_NAME = 'dphi'\n",
    "PREFIX = \"dphi_\"\n",
    "\n",
    "print(NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ba985c-4d58-4f83-bbc5-24de029ba98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings(pandas_version='1.5.3', sklearn_version='1.2.2', numpy_version='1.24.2', base_python_image='python:3.10.11', xgboost_version='1.7.5', evidently_version='0.2.8')\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Settings = namedtuple('Settings', [\n",
    "    'pandas_version',\n",
    "    'sklearn_version',\n",
    "    'numpy_version',\n",
    "    'base_python_image',\n",
    "    'xgboost_version',\n",
    "    'evidently_version'\n",
    "])\n",
    "\n",
    "settings = Settings(\n",
    "    pandas_version=\"1.5.3\",\n",
    "    sklearn_version=\"1.2.2\",\n",
    "    numpy_version=\"1.24.2\",\n",
    "    base_python_image=\"python:3.10.11\",\n",
    "    xgboost_version=\"1.7.5\",\n",
    "    evidently_version=\"0.2.8\"\n",
    ") \n",
    "print(f\"{settings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ca6b3f-aed2-4ecd-ae08-b595bd577d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kfp dsl components\n",
    "import kfp.dsl as dsl\n",
    "from functools import partial\n",
    "from kfp.dsl import pipeline, ContainerOp\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5a8511-7857-4639-a52e-7ff8480ff279",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_download_component.yaml\",\n",
    "    base_image=settings.base_python_image, \n",
    "    packages_to_install=[\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"numpy=={settings.numpy_version}\",\n",
    "        f\"scikit-learn=={settings.sklearn_version}\"\n",
    "    ] \n",
    ")\n",
    "def download_data(output_path: OutputPath(\"CSV\")):\n",
    "\n",
    "    import json\n",
    "    import argparse\n",
    "    from pathlib import Path\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    # Gets and split dataset\n",
    "    url='https://drive.google.com/file/d/1e83nTKyy9UiiparbtiLMTbF1HC40lr0r/view?usp=share_link'\n",
    "    url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "    df = pd.read_csv(url)\n",
    "    df[['grade','view','waterfront']] = df[['grade','view','waterfront']].astype('object')\n",
    "\n",
    "    # Delete entry with 33 bedrooms\n",
    "    df = df[df[\"bedrooms\"] != 33]\n",
    "\n",
    "    features = ['sqft_living','grade', 'sqft_above', 'sqft_living15',\n",
    "           'bathrooms','view','sqft_basement','lat','long','waterfront',\n",
    "           'yr_built', 'bedrooms']\n",
    "    \n",
    "    #creating two datasets for evidently.ai API\n",
    "    ref_data = df[:15000]\n",
    "    prod_data = df[15000:]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(ref_data[features], ref_data['price'], test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "    categorical = ['grade', 'view', 'waterfront']\n",
    "    ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "    ohe = ohe.fit(X_train[categorical])\n",
    "    \n",
    "    def preprocessing(X, y, ohe):\n",
    "        # Convert grade, view, waterfront to type object\n",
    "        X[['grade','view','waterfront']] = X[['grade','view','waterfront']].astype('object')\n",
    "        \n",
    "        # log transform the target varibale \n",
    "        y = np.log1p(y)\n",
    "        \n",
    "        # define categorical and numerical varibales \n",
    "        categorical = ['grade', 'view', 'waterfront']\n",
    "        numerical = ['sqft_living', 'sqft_above', 'sqft_living15',\n",
    "            'bathrooms','sqft_basement','lat','long','yr_built',\n",
    "            'bedrooms']\n",
    "        \n",
    "        # one-hot encode categorical variables\n",
    "        X_cat = ohe.transform(X[categorical]).toarray()\n",
    "        \n",
    "        # define numerical columns \n",
    "        X_num = np.array(X[numerical])\n",
    "        \n",
    "        # concatenate numerical and categorical variables\n",
    "        X = np.concatenate([X_cat, X_num], axis=1)\n",
    "        \n",
    "        print('Shape after one-hot encoding')\n",
    "        print(f'X shape: {X.shape}')\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    X_train, y_train = preprocessing(X_train, y_train, ohe)\n",
    "    X_val, y_val = preprocessing(X_val, y_val, ohe)\n",
    "    X_prod, y_prod = preprocessing(prod_data[features], prod_data['price'], ohe)\n",
    "\n",
    "    X_train_full, y_train_full = preprocessing(ref_data[features], ref_data['price'], ohe)\n",
    "\n",
    "    ref_data_numpy = ref_data.to_numpy()\n",
    "    prod_data_numpy = prod_data.to_numpy()\n",
    "\n",
    "    # Creates `data` structure to save and \n",
    "    # share train, val and prod datasets.\n",
    "    data = {'x_train' : X_train.tolist(),\n",
    "            'y_train' : y_train.tolist(),\n",
    "            'x_train_full' : X_train_full.tolist(),\n",
    "            'y_train_full' : y_train_full.tolist(),\n",
    "            'x_val' : X_val.tolist(),\n",
    "            'y_val' : y_val.tolist(),\n",
    "            'x_prod' : X_prod.tolist(),\n",
    "            'y_prod' : y_prod.tolist(),\n",
    "            'ref_data' : ref_data_numpy.tolist(),\n",
    "            'prod_data' : prod_data_numpy.tolist()}\n",
    "\n",
    "    # Creates a json object based on `data`\n",
    "    data_json = json.dumps(data)\n",
    "    \n",
    "    with open(output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1d7aa9-b8e6-4770-b7ee-70e2c6bd8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}_extremegboost_component.yaml\",\n",
    "    base_image=settings.base_python_image, \n",
    "    packages_to_install=[\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"numpy=={settings.numpy_version}\",\n",
    "        f\"scikit-learn=={settings.sklearn_version}\",\n",
    "        f\"xgboost=={settings.xgboost_version}\"\n",
    "    ] \n",
    ")\n",
    "def process_data(input_path: InputPath(\"CSV\"), output_path: OutputPath(\"CSV\")):\n",
    "    import json\n",
    "    import argparse\n",
    "    from pathlib import Path\n",
    "\n",
    "    import xgboost as xgb\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    # Open and reads file \"data\"\n",
    "    with open(input_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # The excted data type is 'dict', however since the file\n",
    "    # was loaded as a json object, it is first loaded as a string\n",
    "    # thus we need to load again from such string in order to get \n",
    "    # the dict-type object.\n",
    "    data = json.loads(data)\n",
    "\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    x_val = data['x_val']\n",
    "    y_val = data['y_val']\n",
    "\n",
    "    \n",
    "    # Initialize XGB with objective function\n",
    "    parameters = {\"objective\": 'reg:squarederror',\n",
    "                \"n_estimators\": 100,\n",
    "                \"verbosity\": 0}\n",
    "\n",
    "    model = xgb.XGBRegressor(**parameters)\n",
    "    model.fit(x_train, y_train)\n",
    "        \n",
    "    # generate predictions\n",
    "    y_pred_train = model.predict(x_train).reshape(-1,1)\n",
    "    y_pred = model.predict(x_val).reshape(-1,1)\n",
    "        \n",
    "    # calculate errors\n",
    "    rmse_train = mean_squared_error(y_pred_train, y_train, squared=False)\n",
    "    rmse_val = mean_squared_error(y_pred, y_val, squared=False)\n",
    "    print(f\"rmse training: {rmse_train:.3f}\\t rmse validation: {rmse_val:.3f}\")\n",
    "\n",
    "    \n",
    "    #Extracting ref and prod from 'data'\n",
    "    ref_data = data['ref_data']\n",
    "    prod_data = data['prod_data']\n",
    "    X_train_full = data['x_train_full']\n",
    "    X_prod = data['x_prod']\n",
    "\n",
    "    column_names = [\"id\", \"data\", \"price\", \"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"...\", \"grade\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"zipcode\", \"lat\", \"long\", \"sqft_living15\", \"sqft_lot15\"]\n",
    "\n",
    "    #ref_data and prod_data was stored as numpy.darray, therefore, converting back to pandas DataFrame.\n",
    "    ref_data = pd.DataFrame.from_records(ref_data)\n",
    "    ref_data.columns = column_names\n",
    "\n",
    "    prod_data = pd.DataFrame.from_records(prod_data)\n",
    "    prod_data.columns = column_names\n",
    "\n",
    "    ref_data['prediction'] = model.predict(X_train_full)\n",
    "    prod_data['prediction'] = model.predict(X_prod)\n",
    "    ref_data['price_log'] = np.log1p(ref_data['price'])\n",
    "    prod_data['price_log'] = np.log1p(prod_data['price'])\n",
    "\n",
    "    ref_data_final_numpy = ref_data.to_numpy()\n",
    "    prod_data_final_numpy = prod_data.to_numpy()\n",
    "\n",
    "    evidently_data = {\n",
    "        'ref_data' : ref_data_final_numpy.tolist(),\n",
    "        'prod_data' : prod_data_final_numpy.tolist()\n",
    "    }\n",
    "\n",
    "    # Creates a json object based on `evidently_data`\n",
    "    evidently_data_json = json.dumps(evidently_data)\n",
    "        \n",
    "    with open(output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        json.dump(evidently_data_json, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a344b3bb-9282-45a9-b194-722bb0a5d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}evidently_monitoring_component.yaml\",\n",
    "    base_image=settings.base_python_image, # use python base image\n",
    "    packages_to_install=[\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"evidently=={settings.evidently_version}\"\n",
    "    ]    \n",
    ")\n",
    "def evidently_monitoring(intput_path: InputPath(), mlpipeline_ui_metadata_path: OutputPath(str)):\n",
    "    \n",
    "    import json\n",
    "    import argparse\n",
    "    from pathlib import Path\n",
    "    from collections import namedtuple\n",
    "    import os\n",
    "\n",
    "    import pandas as pd\n",
    "    \n",
    "    import evidently\n",
    "\n",
    "    from evidently.dashboard import Dashboard\n",
    "    from evidently.pipeline.column_mapping import ColumnMapping\n",
    "    # packages for interactive dashboards\n",
    "    from evidently.dashboard.tabs import DataDriftTab\n",
    "    \n",
    "        # Open and reads file \"data\"\n",
    "    with open(intput_path) as data_file:\n",
    "        evidently_data = json.load(data_file)\n",
    "\n",
    "    evidently_data = json.loads(evidently_data)\n",
    "\n",
    "    ref_data = evidently_data['ref_data']\n",
    "    prod_data = evidently_data['prod_data']\n",
    "\n",
    "    column_names = [\"id\", \"data\", \"price\", \"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"...\", \"grade\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"zipcode\", \"lat\", \"long\", \"sqft_living15\", \"sqft_lot15\", \"prediction\", \"price_log\"]\n",
    "\n",
    "    ref_data = pd.DataFrame.from_records(ref_data)\n",
    "    ref_data.columns = column_names\n",
    "\n",
    "    prod_data = pd.DataFrame.from_records(prod_data)\n",
    "    prod_data.columns = column_names\n",
    "\n",
    "\n",
    "    # Define the data drift dashboard using the DataDriftTab.\n",
    "    column_mapping = ColumnMapping()\n",
    "    target = 'price_log'\n",
    "    numerical_features = ['sqft_living', 'sqft_above', 'sqft_living15',\n",
    "            'bathrooms','sqft_basement','lat','long',\n",
    "            'yr_built','bedrooms']\n",
    "    categorical_features = ['grade', 'view', 'waterfront']\n",
    "    column_mapping.target = target\n",
    "    column_mapping.prediction = 'prediction'\n",
    "    column_mapping.numerical_features = numerical_features\n",
    "    column_mapping.categorical_features = categorical_features\n",
    "\n",
    "    print(column_mapping)\n",
    "\n",
    "    data_drift_dashboard = Dashboard(tabs=[DataDriftTab(verbose_level=1)])\n",
    "    data_drift_dashboard.calculate(ref_data, prod_data, column_mapping=column_mapping)\n",
    "        \n",
    "    data_drift_dashboard_filename = \"data_drift.html\"\n",
    "    local_dir = \"/tmp/artifact_downloads\"\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.mkdir(local_dir)\n",
    "    static_html_path = os.path.join(local_dir, data_drift_dashboard_filename)\n",
    "    data_drift_dashboard.save(static_html_path)\n",
    "    with open(static_html_path, \"r\") as f:\n",
    "        inline_report = f.read()\n",
    "        \n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": inline_report,\n",
    "                \"type\": \"web-app\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "        \n",
    "    from collections import namedtuple\n",
    "        \n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1969fb-8774-465a-85cc-1591d21dcbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    ")\n",
    "def custom_pipeline():\n",
    "    \n",
    "    '''pipeline'''   \n",
    "    download_task = download_data()\n",
    "    download_task.set_display_name(\"download data\")\n",
    "    \n",
    "    # variable name \"output_path\", all \"_path\" will be removed by sysem\n",
    "    process_data_task = process_data(download_task.outputs[\"output\"])\n",
    "    process_data_task.set_display_name(\"extreme_gboost\")\n",
    "    \n",
    "    visualization_task = evidently_monitoring(process_data_task.outputs[\"output\"])\n",
    "    visualization_task.set_display_name(\"evidently monitoring\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addf3bb4-21ae-4654-b39b-b9a85f5dbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPE_LINE_FILE_NAME=f\"{PREFIX}_assignment_pipeline\"\n",
    "kfp.compiler.Compiler().compile(custom_pipeline, f\"{PIPE_LINE_FILE_NAME}.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139979d-707f-412e-b295-f17e557576ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df441f82-d516-47a0-9e6d-425abc152116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
